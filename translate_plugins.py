# translate_plugins.py
import os
import re
import json
import time
import requests
import zipfile
import io
import hashlib
import subprocess
import shutil

# ----------------------------
# é…ç½®
# ----------------------------
MAIN_ZIP_URL = "https://dl.awa.cool/hahappify/xlcj/qun.zip"
REMOTE_DICT_U_URL = "https://raw.githubusercontent.com/SwitchScriptTW/More/refs/heads/main/dict_url.json"
# REMOTE_DICT_S_URL = "https://raw.githubusercontent.com/SwitchScriptTW/More/refs/heads/main/dict_string.json"
TEMP_DIR = "./temp"           # è‡¨æ™‚ä¸‹è¼‰èˆ‡è§£å£“
OUTPUT_DIR_HANS = "./Hans"    # åŸå§‹ç°¡é«” ZIP
# OUTPUT_DIR_HANT = "./Hant"    # ç¹é«” ZIP
RELEASES_DIR = "./releases"      # ç¿»è­¯å¾Œ ZIP æª”æ¡ˆ (ç”¨æ–¼ Releases)

DICT_STRING_FILE = "./dict_string.json"
DICT_URL_FILE = "./dict_url.json"

# ----------------------------
# è¼”åŠ©å‡½æ•¸
# ----------------------------
def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def file_hash(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        h.update(f.read())
    return h.hexdigest()

def download_file(url):
    print(f"Downloading {url}")
    time.sleep(30)
    r = requests.get(url)
    r.raise_for_status()
    return r.content

def zhconvert(text, lang="Taiwan"):
    # è©èªæ¨¡çµ„
    modules = '{"Computer":1,"Smooth":1,"Unit":1,"ProperNoun":1,"QuotationMark":1,"InternetSlang":1,"Repeat":1,"RepeatAutoFix":1,"GanToZuo":0}'
    # ä¿è­·å­—è©
    userProtectReplace = "ç”¨æˆ¶"
    # è½‰æ›å‰æ›¿æ›
    userPreReplace = "æ’ä»¶=å¤–æ›"
    # è½‰æ›å¾Œæ›¿æ›
    userPostReplace = "ç²å–=å–å¾—\næ·»åŠ =æ–°å¢\nä¸‹åŠƒç·š=åº•ç·š\nç›¸å†Š=ç›¸ç°¿"
    args = {
        "text": text,
        "converter": lang,
        "modules": modules,
        "userPreReplace": userPreReplace,
        "userPostReplace": userPostReplace,
        "userProtectReplace": userProtectReplace
    }
    url = "https://api.zhconvert.org/convert"
    response = requests.post(url, data=args, headers={'User-Agent': 'SwitchScriptTW_Bot/1.0 (+https://github.com/david082321)'}).content.decode("utf8")
    try:
        code = json.loads(response)["code"]
        if code == 0:
            return json.loads(response)["data"]["text"]
        else:
            print("Error:", response)
            return text
    except:
        print("Error:", response)
        return text

def extract_zip(content, extract_to):
    ensure_dir(extract_to)
    z = zipfile.ZipFile(io.BytesIO(content))
    z.extractall(extract_to)
    return [f.filename for f in z.infolist() if not f.is_dir()]

def zip_dir(folder_path, zip_path):
    ensure_dir(os.path.dirname(zip_path))
    with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
        for root, _, files in os.walk(folder_path):
            for f in files:
                fullpath = os.path.join(root, f)
                arcname = os.path.relpath(fullpath, folder_path)
                z.write(fullpath, arcname)

def load_json(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf8") as f:
            return json.load(f)
    return {}

def save_json(path, obj):
    ensure_dir(os.path.dirname(path))
    with open(path, "w", encoding="utf8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def line_contains_chinese(line):
    return re.search(r"[\u4e00-\u9fa5]", line) is not None

def load_etag(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf8") as f:
            return f.read().strip()
    return None

def save_etag(path, etag):
    ensure_dir(os.path.dirname(path))
    with open(path, "w", encoding="utf8") as f:
        f.write(etag)

# ----------------------------
# ä¸»ç¨‹å¼
# ----------------------------
def main():
    ensure_dir(TEMP_DIR)
    ensure_dir(OUTPUT_DIR_HANS)
    # ensure_dir(OUTPUT_DIR_HANT)
    ensure_dir(RELEASES_DIR)
    ensure_dir("./translation")
    ensure_dir("./dict")

    dict_string = load_json(DICT_STRING_FILE)
    dict_url = load_json(DICT_URL_FILE)
    # å¾ GitHub å–å¾—æœ€æ–° dict_url.json
    try:
        print("Fetching remote dict_url.json ...")
        r = requests.get(REMOTE_DICT_U_URL, timeout=10)
        r.raise_for_status()
        dict_url_remote = r.json()
        dict_url.update(dict_url_remote)   # ç”¨é ç«¯æ›´æ–°æœ¬åœ° dict_url
        print("Loaded remote dict_url.json")
    except Exception as e:
        print(f"Failed to fetch remote dict_url.json: {e}")

    black_url = []
    black_zip = [
        "emuiibo.zip", # å·²æœ‰ç¹é«”
        "ftpsrv.zip", # ç„¡ç°¡é«”ç¿»è­¯
        "KeyX.zip", # å·²æœ‰ç¹é«”
        "dvr-patches.zip", # ä¸éœ€è¦ç¿»è­¯
        "DBI.zip", # ä¸ä½¿ç”¨ï¼Œæ”¹ç”¨ ssky çš„ç‰ˆæœ¬
        "Breeze.zip", # ç„¡ç°¡é«”ç¿»è­¯
        "AtmoXL-Titel-Installer.zip", # å·²æœ‰ç¹é«”
        "BBI.zip", # ç„¡ç°¡é«”ç¿»è­¯

        "wiliwili.zip", # å·²æœ‰ç¹é«”
        "Goldleaf.zip", # å·²æœ‰ç¹é«”
        "aio-switch-updater.zip", # å·²æœ‰ç¹é«”
        "battery_desync_fix.zip", # ç„¡ç°¡é«”ç¿»è­¯
        "SwitchThemesNX.zip", # ä¸è™•ç†ï¼Œå­—é«”å•é¡Œ
        "PPSSPP.zip", # å·²æœ‰ç¹é«”

        "NX-Activity-Log.zip", # å·²æœ‰ç¹é«”ï¼Œä½†éœ€è¦ä¿®æ­£
        "NX-Mod-Manager.zip",  # å·²æœ‰ç¹é«”ï¼Œä½†éœ€è¦ä¿®æ­£
    ]
    balck_file = [
        "Fizeau.nro",
        "DClight.ovl",
        "SysDVR.nro",
    ]

    # ----------------------------
    # æ‰¾å‡ºå…§éƒ¨ URL
    # ----------------------------
    url_set = set()
    for k in dict_url.keys():
        if k.startswith("https://dl.awa.cool/hahappify/nro/") and k not in black_url:
            url_set.add(k)

    # ----------------------------
    # ä¸‹è¼‰æ‰€æœ‰ URL ä¸¦ç¹åŒ–
    # ----------------------------
    for url in url_set:
        print(f"\nè®€å–ç¶²å€: {url}")
        url_path = url.replace("https://dl.awa.cool/", "")
        local_path_hans = os.path.join(OUTPUT_DIR_HANS, url_path)
        ensure_dir(os.path.dirname(local_path_hans))

        etag_file = local_path_hans + ".etag"
        etag_local = load_etag(etag_file)

        # åˆ¤æ–·æ˜¯å¦éœ€è¦ä¸‹è¼‰
        need_download = True
        etag_remote = None

        need_download = False
        # try:
        #     head_resp = requests.head(url, timeout=15)
        #     etag_remote = head_resp.headers.get("ETag")
        #     if etag_remote:
        #         etag_remote = etag_remote.strip('"')  # å»æ‰é›™å¼•è™Ÿ
        #         if etag_remote == etag_local:
        #             print("ç„¡æ›´æ–°ï¼Œè·³éä¸‹è¼‰")
        #             time.sleep(30) # é¿å…éå¿«é‡è¤‡è«‹æ±‚
        #             need_download = False
        # except Exception as e:
        #     print(f"HEAD request failed: {e}, will download")
        
        # ä¸‹è¼‰ ZIP
        if need_download:
            try:
                content = download_file(url)
                # å„²å­˜ ETag
                if etag_remote:
                    save_etag(etag_file, etag_remote)
            except Exception as e:
                print(f"Download failed: {e}")
                continue
            # ä¿å­˜åŸå§‹ç°¡é«”åˆ° Hans
            with open(local_path_hans, "wb") as f:
                f.write(content)
            print(f"Saved original ZIP: {local_path_hans}")
        else:
            with open(local_path_hans, "rb") as f:
                content = f.read()

        # å–å¾— ZIP æª”æ¡ˆåç¨± (ä¾‹å¦‚ DBI.zip)
        zip_filename = os.path.basename(local_path_hans) 

        # æ’é™¤ä¸éœ€è™•ç†çš„ zip
        if zip_filename in black_zip:
            # zip è¤‡è£½åˆ° releases
            release_zip_path = os.path.join(RELEASES_DIR, url_path) # ./releases/hahappify/nro/DBI.zip
            ensure_dir(os.path.dirname(release_zip_path))
            shutil.copy2(local_path_hans, release_zip_path)
            print(f"âœ… å„²å­˜åˆ° {release_zip_path}")
            continue

        # è¨­å®š temp è³‡æ–™å¤¾è·¯å¾‘ï¼š
        # é€™è£¡æˆ‘å€‘å…ˆè§£å£“åˆ°ä¸€å€‹è‡¨æ™‚ç›®éŒ„ï¼Œç„¶å¾Œå†ç§»å‹•åˆ°æ‚¨æŒ‡å®šçš„çµæ§‹ã€‚
        temp_extract_dir = os.path.join(TEMP_DIR, zip_filename + "_extract") # ä½¿ç”¨ä¸€å€‹è‡¨æ™‚è§£å£“ç›®éŒ„
        extract_zip(content, temp_extract_dir)

        # æ ¹æ“šæ‚¨çš„æ–°çµæ§‹ï¼Œå®šç¾©æœ€çµ‚çš„ temp è·¯å¾‘
        # url_path: hahappify/nro/DBI.zip
        final_temp_path = os.path.join(TEMP_DIR, url_path + "/")
        
        # å°‡è§£å£“å…§å®¹ç§»å‹•åˆ° final_temp_path
        # å‡è¨­ ZIP å…§å®¹æ²’æœ‰é ‚å±¤è³‡æ–™å¤¾
        if os.path.exists(final_temp_path):
            shutil.rmtree(final_temp_path)
        shutil.move(temp_extract_dir, final_temp_path) # å°‡è§£å£“å…§å®¹ç§»è‡³æ–°çµæ§‹è·¯å¾‘
        
        # è¨­å®šå¾ŒçºŒè™•ç†çš„ç›®éŒ„ç‚º final_temp_path
        temp_dir_for_processing = final_temp_path

        # è™•ç†æ¯å€‹æ–‡å­—æª”
        for root, _, files in os.walk(temp_dir_for_processing):
            for f in files:
                path = os.path.join(root, f)
                if f.lower() == "zh-hans.json":
                    continue  # è·³éç°¡é«”å­—å…¸æª”
                try:
                    with open(path, "r", encoding="utf8") as file:
                        lines = file.readlines()
                    new_lines = []
                    for line in lines:
                        # æ›¿æ› URL
                        def replace_url(m):
                            url = m.group(0)
                            if url not in dict_url:
                                dict_url[url] = url  # é è¨­ value ç­‰æ–¼åŸ URL
                            return dict_url[url]        
                        line = re.sub(r"https://dl\.awa\.cool/[^\s\"']+", replace_url, line)

                        # ç¹åŒ–ä¸­æ–‡
                        if line_contains_chinese(line):
                            if line in dict_string:
                                new_line = dict_string[line]
                            else:
                                new_line = zhconvert(line)
                                dict_string[line] = new_line
                                time.sleep(1)
                            new_lines.append(new_line)
                        else:
                            new_lines.append(line)
                    with open(path, "w", encoding="utf8") as file:
                        file.writelines(new_lines)
                except:
                    continue

        # ----------------------------
        # è‡ªå‹•ç¿»è­¯ *.nro / *.ovl
        # ----------------------------
        for root, _, files in os.walk(temp_dir_for_processing):
            for f in files:
                path = os.path.join(root, f)
                if path.lower().endswith((".nro", ".ovl")) and f not in balck_file:
                    print(f"ğŸ”„ æ­£åœ¨ç¿»è­¯ {f} ...")
                    subprocess.run([
                        "python", "translate_nro.py", path
                    ], check=True)

        # ä¿å­˜ dict_url.json
        if url not in dict_url:
            dict_url[url] = url
        save_json(DICT_STRING_FILE, dict_string)
        save_json(DICT_URL_FILE, dict_url)

        # ----------------------------
        # å°‡è™•ç†å¾Œçš„æª”æ¡ˆå¾ Temp è¤‡è£½/ç§»å‹•åˆ° Hant
        # ----------------------------
        # hant_folder_path = os.path.join(OUTPUT_DIR_HANT, url_path + "/") # ./Hant/hahappify/nro/DBI.zip/
        # ensure_dir(os.path.dirname(hant_folder_path))
        # if os.path.exists(hant_folder_path):
        #     shutil.rmtree(hant_folder_path) # å…ˆåˆªé™¤èˆŠçš„ Hant è³‡æ–™å¤¾
        # shutil.copytree(temp_dir_for_processing, hant_folder_path) # è¤‡è£½åˆ° Hant
        # print(f"âœ… Copied translated files to Hant folder: {hant_folder_path}")

        # ----------------------------
        # å£“ç¸®å› ZIP (Releases)
        # ----------------------------
        # zip_dir(folder_path, zip_path)
        release_zip_path = os.path.join(RELEASES_DIR, url_path) # ./releases/hahappify/nro/DBI.zip
        zip_dir(temp_dir_for_processing, release_zip_path) # <--- å¾è™•ç†å¾Œçš„ temp è³‡æ–™å¤¾å£“ç¸®
        print(f"ğŸ“¦ å„²å­˜åˆ° {release_zip_path}")

        # ----------------------------
        # æ¸…ç†è‡¨æ™‚è³‡æ–™å¤¾
        # ----------------------------
        shutil.rmtree(temp_dir_for_processing)

if __name__ == "__main__":
    main()
